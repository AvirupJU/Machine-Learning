{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Hello ML\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Hello ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Mean Squared Error is :  1826.5364191345425\nSlope :  [  -1.16924976 -237.18461486  518.30606657  309.04865826 -763.14121622\n  458.90999325   80.62441437  174.32183366  721.49712065   79.19307944]\nIntercept :  153.05827988224112\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Finding Blood Sugar levels to predict diabetes in patients using Linear Regression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "diabetes= datasets.load_diabetes()\n",
    "\n",
    "#diabetes_X= diabetes.data[:,np.newaxis,2]\n",
    "diabetes_X= diabetes.data\n",
    "#print(diabetes_X)\n",
    "diabetes_X_train= diabetes_X[:-30]\n",
    "diabetes_X_test= diabetes_X[-30:]\n",
    "\n",
    "diabetes_Y_train = diabetes.target[:-30]\n",
    "diabetes_Y_test = diabetes.target[-30:]\n",
    "#gradient descent is used to minimise the MSE\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(diabetes_X_train,diabetes_Y_train)\n",
    "\n",
    "diabetes_Y_predict = model.predict(diabetes_X_test)\n",
    "\n",
    "print(\"Mean Squared Error is : \",mean_squared_error(diabetes_Y_test,diabetes_Y_predict))\n",
    "print(\"Slope : \", model.coef_)\n",
    "print(\"Intercept : \", model.intercept_)\n",
    "\n",
    "#plt.scatter(diabetes_X_test,diabetes_Y_test)\n",
    "#plt.plot(diabetes_X_test,diabetes_Y_predict)\n",
    "\n",
    "\n",
    "#plt.show()\n",
    "#Pros: Quite Simple, Fast on small datasets\n",
    "#Cons: Not necessary that a linear model gives optimum accuracy(we need tune tune the hyper parameters for that)\n",
    "#Optimization: Better if we take a small sample from our training dataset as a validation test to check for the overfitting problem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[2]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Iris Classification Problem using KNN Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "#print(features[0],labels[0])\n",
    "\n",
    "clf= KNeighborsClassifier()\n",
    "clf.fit(features,labels)\n",
    "print(clf.predict([[20,10.4,0.2,0.8]]))\n",
    "# Pros: Simple, Works quite well on simple and small datasets\n",
    "#Cons: For large datasets calculation of distances is expensive, Finding an optimum value for K is a daunting task."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[1]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Logistic Regression--logistic curve gives a non zero probability which in turn helps us to classify the test point\n",
    "# Logistic Curves - Sigmoid, Softmax\n",
    "'''We use a line to predict the log of odds. The sigmoid then transforms it into a probability. We can't use MSE\n",
    "   to tune the parameters (weights and biases) since the error becomes non-linear after the transformation so we use the log loss\n",
    "    to calculate the error: Loss=-ylog(y')-(1-y)log(1-y'). This model is very good for binary classification.'''\n",
    "\n",
    "# We take the same problem of iris classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X= iris[\"data\"]\n",
    "Y= (iris[\"target\"]==2).astype(np.int)\n",
    "#print(X)\n",
    "#print(Y)\n",
    "#training logistic regeression model\n",
    "clf= LogisticRegression()\n",
    "clf.fit(X,Y)\n",
    "test_val= clf.predict([[5.9, 3.0,  5.1, 1.8]])\n",
    "print(test_val)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}